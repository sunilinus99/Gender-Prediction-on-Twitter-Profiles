{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week07-TextMining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "\n",
    "This notebook explores the use of machine learning models to predict the gender of Twitter account holders based on their profile descriptions from the dataset provided on data.world website against the link: https://data.world/crowdflower/gender-classifier-data. The dataset contains 20,050 rows, with columns like user name, a random tweet, account profile, location, etc, total of 26 columns which gives the information about the user of a given twitter handle.\n",
    "\n",
    "The objective of this project is to create a predictive model that, using the descriptions of users' profiles, can reliably identify the gender of Twitter users. To determine which machine learning method is best for this task, we will look into a number of options, such as AdaBoost, XGBoost, K-Nearest Neighbors (KNN), Decision Tree, Support Vector Machine (SVM), and Logistic Regression.\n",
    "\n",
    "First, we will preprocess the dataset using methods like lemmatization for text preprocessing, Singular Value Decomposition (SVD) and other data prep techniques. After that, the dataset will be divided into training and testing sets. Next, in order to enhance each model's performance, we will train and evaluate it using cross-validation and hyperparameter tuning. Ultimately, the model that performs the best will be chosen based on how accurate it is on the test set.\n",
    "\n",
    "Our goal is to develop a strong machine learning model by the conclusion of this notebook that can identify the gender of Twitter users based on their profile descriptions for the uses that will be listed in the final discussion in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import common packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random_seed = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>gender</th>\n",
       "      <th>gender:confidence</th>\n",
       "      <th>profile_yn</th>\n",
       "      <th>profile_yn:confidence</th>\n",
       "      <th>created</th>\n",
       "      <th>...</th>\n",
       "      <th>profileimage</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>sidebar_color</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_count</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>815719226</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/26/15 23:24</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12/5/13 1:48</td>\n",
       "      <td>...</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/414342229...</td>\n",
       "      <td>0</td>\n",
       "      <td>FFFFFF</td>\n",
       "      <td>Robbie E Responds To Critics After Win Against...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110964</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "      <td>main; @Kan1shk3</td>\n",
       "      <td>Chennai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>815719227</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/26/15 23:30</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10/1/12 13:51</td>\n",
       "      <td>...</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/539604221...</td>\n",
       "      <td>0</td>\n",
       "      <td>C0DEED</td>\n",
       "      <td>ÛÏIt felt like they were my friends and I was...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7471</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>815719228</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/26/15 23:33</td>\n",
       "      <td>male</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11/28/14 11:30</td>\n",
       "      <td>...</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/657330418...</td>\n",
       "      <td>1</td>\n",
       "      <td>C0DEED</td>\n",
       "      <td>i absolutely adore when louis starts the songs...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5617</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "      <td>clcncl</td>\n",
       "      <td>Belgrade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>815719229</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/26/15 23:10</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6/11/09 22:39</td>\n",
       "      <td>...</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/259703936...</td>\n",
       "      <td>0</td>\n",
       "      <td>C0DEED</td>\n",
       "      <td>Hi @JordanSpieth - Looking at the url - do you...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1693</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "      <td>Palo Alto, CA</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>815719230</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/27/15 1:15</td>\n",
       "      <td>female</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4/16/14 13:23</td>\n",
       "      <td>...</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/564094871...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Watching Neighbours on Sky+ catching up with t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31462</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0  815719226    False   finalized                   3    10/26/15 23:24   \n",
       "1  815719227    False   finalized                   3    10/26/15 23:30   \n",
       "2  815719228    False   finalized                   3    10/26/15 23:33   \n",
       "3  815719229    False   finalized                   3    10/26/15 23:10   \n",
       "4  815719230    False   finalized                   3     10/27/15 1:15   \n",
       "\n",
       "   gender  gender:confidence profile_yn  profile_yn:confidence  \\\n",
       "0    male             1.0000        yes                    1.0   \n",
       "1    male             1.0000        yes                    1.0   \n",
       "2    male             0.6625        yes                    1.0   \n",
       "3    male             1.0000        yes                    1.0   \n",
       "4  female             1.0000        yes                    1.0   \n",
       "\n",
       "          created  ...                                       profileimage  \\\n",
       "0    12/5/13 1:48  ...  https://pbs.twimg.com/profile_images/414342229...   \n",
       "1   10/1/12 13:51  ...  https://pbs.twimg.com/profile_images/539604221...   \n",
       "2  11/28/14 11:30  ...  https://pbs.twimg.com/profile_images/657330418...   \n",
       "3   6/11/09 22:39  ...  https://pbs.twimg.com/profile_images/259703936...   \n",
       "4   4/16/14 13:23  ...  https://pbs.twimg.com/profile_images/564094871...   \n",
       "\n",
       "   retweet_count sidebar_color  \\\n",
       "0              0        FFFFFF   \n",
       "1              0        C0DEED   \n",
       "2              1        C0DEED   \n",
       "3              0        C0DEED   \n",
       "4              0             0   \n",
       "\n",
       "                                                text tweet_coord tweet_count  \\\n",
       "0  Robbie E Responds To Critics After Win Against...         NaN      110964   \n",
       "1  ÛÏIt felt like they were my friends and I was...         NaN        7471   \n",
       "2  i absolutely adore when louis starts the songs...         NaN        5617   \n",
       "3  Hi @JordanSpieth - Looking at the url - do you...         NaN        1693   \n",
       "4  Watching Neighbours on Sky+ catching up with t...         NaN       31462   \n",
       "\n",
       "    tweet_created      tweet_id   tweet_location               user_timezone  \n",
       "0  10/26/15 12:40  6.587300e+17  main; @Kan1shk3                     Chennai  \n",
       "1  10/26/15 12:40  6.587300e+17              NaN  Eastern Time (US & Canada)  \n",
       "2  10/26/15 12:40  6.587300e+17           clcncl                    Belgrade  \n",
       "3  10/26/15 12:40  6.587300e+17    Palo Alto, CA  Pacific Time (US & Canada)  \n",
       "4  10/26/15 12:40  6.587300e+17              NaN                         NaN  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/sunilinus/Downloads/text_mining_data.csv', encoding='latin1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20050, 26)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_unit_id', '_golden', '_unit_state', '_trusted_judgments',\n",
       "       '_last_judgment_at', 'gender', 'gender:confidence', 'profile_yn',\n",
       "       'profile_yn:confidence', 'created', 'description', 'fav_number',\n",
       "       'gender_gold', 'link_color', 'name', 'profile_yn_gold', 'profileimage',\n",
       "       'retweet_count', 'sidebar_color', 'text', 'tweet_coord', 'tweet_count',\n",
       "       'tweet_created', 'tweet_id', 'tweet_location', 'user_timezone'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check each columns \n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping unnecessary columns\n",
    "\n",
    "Dropping columns which do not seem directly related to predicting gender based on profile descriptions. Therefore, keeping them would introduce unnecessary complexity to the analysis.\n",
    "\n",
    "Through exclusive attention to the 'gender' and 'description' columns, the study can focus on understanding the relationship between the content of account holder's profile descriptions and their gender. As a result, the task becomes simpler and it is straightforward to interpret the outcomes.\n",
    "\n",
    "Hence dropping all the columns except 'gender' and 'description'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['_unit_id', '_golden', '_unit_state', '_trusted_judgments',\n",
    "       '_last_judgment_at', 'gender:confidence', 'profile_yn',\n",
    "       'profile_yn:confidence', 'created', 'fav_number',\n",
    "       'gender_gold', 'link_color', 'name', 'profile_yn_gold', 'profileimage',\n",
    "       'retweet_count', 'sidebar_color', 'text', 'tweet_coord', 'tweet_count',\n",
    "       'tweet_created', 'tweet_id', 'tweet_location', 'user_timezone'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender           97\n",
       "description    3744\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminating the rows containing null values since it is difficult to impute missing values in the text input-filled \"description\" column. Unlike numerical or categorical data, there is no straightforward way to impute missing text values that would preserve the integrity and context of the data. Also, removing these rows won't likely have a major effect on the modeling process because there aren't that many records in the dataset overall that have missing values. There is still enough data in the remaining dataset to train a reliable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing date values\n",
    "df = df.dropna(subset=['gender', 'description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['male', 'female', 'brand', 'unknown'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['gender'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping the records where the gender is 'unknown' since records where the gender is 'unknown' do not provide any meaningful information for training the model, as the model's objective is to predict the gender of Twitter account holders based on their profile descriptions. The inclusion of these records may result in noise introduction and a possible reduction in the prediction performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop records where 'gender' is 'unknown'\n",
    "df = df.loc[df['gender'] != 'unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imbalance in the dataset:\n",
      "female    0.368831\n",
      "male      0.352339\n",
      "brand     0.278830\n",
      "Name: gender, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate the imbalance in the dataset\n",
    "gender_counts = df['gender'].value_counts()\n",
    "total_samples = len(df)\n",
    "\n",
    "# Calculate the proportion of each gender class\n",
    "imbalance = gender_counts / total_samples\n",
    "\n",
    "print(\"Imbalance in the dataset:\")\n",
    "print(imbalance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not addressing the imbalance since there is no great imbalance observed in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading necessary libraries to run lemmetization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sunilinus/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sunilinus/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Lemmatization function\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "    return ' '.join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lemmatization to 'description' column\n",
    "df['description'] = df['description'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign the input variable to X and the target variable to y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a multi-class classification problem. There are three categories we will predict:<br>\n",
    "Whether a twitter account owner is a 'male' or a 'female' or a 'brand'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['male', 'female', 'brand'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['gender']\n",
    "y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brand' 'female' 'male']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, ..., 2, 1, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y)\n",
    "print(le.classes_)\n",
    "y = le.transform(y)\n",
    "\n",
    "y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data\n",
    "\n",
    "Given the size of the data, splitting it into 70% train and 30% test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10865,), (10865,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4657,), (4657,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13140    Writer of chapter 34 Our Encounters with Madne...\n",
       "10932    just a Puerto Rican thats thicker than a bowl ...\n",
       "1416              Keep your heel , head and standards high\n",
       "16483                                    Forex , Marketing\n",
       "11496                       â¬Ä_Ä â£ÄÄâøâ¢Ä_Ä Ä_\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn: Text preparation\n",
    "\n",
    "In the below code, TfidfVectorizer was used to convert text data from the 'description' column into numerical features that can be used for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TfidfVectorizer includes pre-processing, tokenization, filtering stop words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(stop_words='english', lowercase=True, token_pattern=\"[^\\W\\d_]+\")\n",
    "\n",
    "X_train = tfidf_vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the TfidfVectorizer transformation\n",
    "# Be careful: We are using the train fit to transform the test data set. Otherwise, the test data \n",
    "# features will be very different and match the train set!!!\n",
    "\n",
    "X_test = tfidf_vect.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10865, 24059), (4657, 24059))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10865x24059 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 97578 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These data sets are \"sparse matrix\". We can't see them unless we convert using toarray()\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These data sets are \"sparse matrix\". We can't see them unless we convert using toarray()\n",
    "X_train.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis (Singular Value Decomposition)\n",
    "\n",
    "The purpose of its application is to reduce the dimensionality of the text data in the 'description' column by preprocessing it and extracting significant features that can be trained into machine learning models to predict the gender of Twitter account holders based on their profile descriptions, ultimately to reduce the dimensionality of the text data in the 'description' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=300, n_iter=10) #n_components is the number of topics, which should be less than the number of features\n",
    "\n",
    "X_train= svd.fit_transform(X_train)\n",
    "X_test = svd.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10865, 300), (4657, 300))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dataset is preprocessed and ready to train and evaluate various machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, max_leaf_nodes=16, n_jobs=-1) \n",
    "_ = rnd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc (Random Forest): 0.5566\n"
     ]
    }
   ],
   "source": [
    "#Train accuracy - Not a good measure of model performance as we are using the same data set to train and test\n",
    "y_pred_train_rf = rnd_clf.predict(X_train)\n",
    "train_acc_rf = accuracy_score(y_train, y_pred_train_rf)\n",
    "print(f\"Train acc (Random Forest): {accuracy_score(y_train, y_pred_train_rf):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc (Random Forest): 0.5353\n"
     ]
    }
   ],
   "source": [
    "#Test accuracy\n",
    "y_pred_test_rf = rnd_clf.predict(X_test)\n",
    "test_acc_rf = accuracy_score(y_test, y_pred_test_rf)\n",
    "print(f\"Train acc (Random Forest): {accuracy_score(y_test, y_pred_test_rf):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Logistic Regression):\n",
      "[[ 652  336  303]\n",
      " [ 116 1237  375]\n",
      " [ 161  873  604]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix_rf = confusion_matrix(y_test, y_pred_test_rf)\n",
    "print(\"Confusion Matrix (Logistic Regression):\")\n",
    "print(confusion_matrix_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning on Random Forest:\n",
    "\n",
    "n_estimators: The number of trees in the forest. We will test values of 50, 100, and 150 to see which number of trees provides the best performance without overfitting or underfitting.\n",
    "\n",
    "max_depth: The maximum depth of the trees. We will test values of 5, 10, and 15 to control the complexity of the model and avoid overfitting.\n",
    "\n",
    "min_samples_split: The minimum number of samples required to split an internal node. We will test values of 2, 5, and 10 to determine the optimal value for splitting nodes.\n",
    "\n",
    "min_samples_leaf: The minimum number of samples required to be at a leaf node. We will test values of 1, 2, and 4 to control overfitting by setting a minimum threshold for samples at leaf nodes.\n",
    "\n",
    "max_features: The number of features to consider when looking for the best split. We will test values of 'sqrt', 'log2', and None (considering all features) to determine the optimal number of features to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "{'max_depth': 15, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 150}\n",
      "Test acc (Best Random Forest): 0.5832\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Create the Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform Grid Search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "y_pred_test_best = best_rf_model.predict(X_test)\n",
    "test_acc_best = accuracy_score(y_test, y_pred_test_best)\n",
    "print(f\"Test acc (Best Random Forest): {test_acc_best:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create and train the Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "_ = log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc (Logistic Regression): 0.6075\n"
     ]
    }
   ],
   "source": [
    "y_pred_train_lr = log_reg.predict(X_train)\n",
    "train_acc_lr = accuracy_score(y_train, y_pred_train_lr)\n",
    "print(f\"Train acc (Logistic Regression): {train_acc_lr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc (Logistic Regression): 0.5905\n"
     ]
    }
   ],
   "source": [
    "y_pred_test_lr = log_reg.predict(X_test)\n",
    "test_acc_lr = accuracy_score(y_test, y_pred_test_lr)\n",
    "print(f\"Test acc (Logistic Regression): {test_acc_lr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Logistic Regression):\n",
      "[[ 775  257  259]\n",
      " [ 128 1168  432]\n",
      " [ 171  660  807]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix for Logistic Regression\n",
    "confusion_matrix_lr = confusion_matrix(y_test, y_pred_test_lr)\n",
    "print(\"Confusion Matrix (Logistic Regression):\")\n",
    "print(confusion_matrix_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning on Logistic Regression:\n",
    "\n",
    "C: Inverse of regularization strength. Smaller values specify stronger regularization. We will test values of 0.1, 1, and 10 to find the optimal regularization strength.\n",
    "\n",
    "penalty: The norm used in the penalization. We will test 'l1' (Lasso) and 'l2' (Ridge) penalties to see which performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sunilinus/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
      "9 fits failed out of a total of 18.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "9 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sunilinus/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/sunilinus/anaconda3/lib/python3.11/site-packages/sklearn/base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sunilinus/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sunilinus/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/sunilinus/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan 0.55204651        nan 0.57652914        nan 0.57395239]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "{'C': 1, 'penalty': 'l2'}\n",
      "Test acc (Best Logistic Regression): 0.5905\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "# Create the Logistic Regression model\n",
    "log_reg_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# Perform Grid Search to find the best hyperparameters\n",
    "grid_search_lr = GridSearchCV(estimator=log_reg_model, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "grid_search_lr.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(grid_search_lr.best_params_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_lr_model = grid_search_lr.best_estimator_\n",
    "y_pred_test_lr_best = best_lr_model.predict(X_test)\n",
    "test_acc_lr_best = accuracy_score(y_test, y_pred_test_lr_best)\n",
    "print(f\"Test acc (Best Logistic Regression): {test_acc_lr_best:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create and train the KNN model\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "_ = knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc (KNN): 0.6574\n"
     ]
    }
   ],
   "source": [
    "y_pred_train_knn = knn.predict(X_train)\n",
    "train_acc_knn = accuracy_score(y_train, y_pred_train_knn)\n",
    "print(f\"Train acc (KNN): {train_acc_knn:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc (KNN): 0.4904\n"
     ]
    }
   ],
   "source": [
    "y_pred_test_knn = knn.predict(X_test)\n",
    "test_acc_knn = accuracy_score(y_test, y_pred_test_knn)\n",
    "print(f\"Test acc (KNN): {test_acc_knn:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (KNN):\n",
      "[[709 293 289]\n",
      " [316 924 488]\n",
      " [362 625 651]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix for KNN\n",
    "confusion_matrix_knn = confusion_matrix(y_test, y_pred_test_knn)\n",
    "print(\"Confusion Matrix (KNN):\")\n",
    "print(confusion_matrix_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning on KNN:\n",
    "\n",
    "n_neighbors: The number of neighbors to consider. We will test values of 3, 5, and 7 to find the optimal number of neighbors.\n",
    "\n",
    "weights: The weight function used in prediction. We will test 'uniform' and 'distance' to see which performs better.\n",
    "\n",
    "p: The power parameter for the Minkowski distance. We will test values of 1 (Manhattan distance) and 2 (Euclidean distance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "{'n_neighbors': 7, 'p': 2, 'weights': 'distance'}\n",
      "Test acc (Best KNN): 0.5123\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': [1, 2]\n",
    "}\n",
    "\n",
    "# Create the KNN model\n",
    "knn_model = KNeighborsClassifier()\n",
    "\n",
    "# Perform Grid Search to find the best hyperparameters\n",
    "grid_search_knn = GridSearchCV(estimator=knn_model, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "grid_search_knn.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(grid_search_knn.best_params_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_knn_model = grid_search_knn.best_estimator_\n",
    "y_pred_test_knn_best = best_knn_model.predict(X_test)\n",
    "test_acc_knn_best = accuracy_score(y_test, y_pred_test_knn_best)\n",
    "print(f\"Test acc (Best KNN): {test_acc_knn_best:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create and train the SVM model\n",
    "svm = SVC(kernel='linear')\n",
    "_ = svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc (SVM): 0.5983\n"
     ]
    }
   ],
   "source": [
    "y_pred_train_svm = svm.predict(X_train)\n",
    "train_acc_svm = accuracy_score(y_train, y_pred_train_svm)\n",
    "print(f\"Train acc (SVM): {train_acc_svm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc (SVM): 0.5738\n"
     ]
    }
   ],
   "source": [
    "y_pred_test_svm = svm.predict(X_test)\n",
    "test_acc_svm = accuracy_score(y_test, y_pred_test_svm)\n",
    "print(f\"Test acc (SVM): {test_acc_svm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (SVM):\n",
      "[[ 694  314  283]\n",
      " [ 112 1243  373]\n",
      " [ 145  758  735]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix for SVM\n",
    "confusion_matrix_svm = confusion_matrix(y_test, y_pred_test_svm)\n",
    "print(\"Confusion Matrix (SVM):\")\n",
    "print(confusion_matrix_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning on SVM:\n",
    "\n",
    "C: Regularization parameter. We will test values of 0.1, 1, and 10 to find the optimal regularization strength.\n",
    "\n",
    "kernel: Kernel type to be used in the algorithm. We will test 'linear', 'rbf', and 'sigmoid' kernels to find the best kernel for our model.\n",
    "\n",
    "gamma: Kernel coefficient for 'rbf' and 'sigmoid'. We will test values of 'scale' and 'auto' to determine the optimal gamma value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "{'C': 1, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Test acc (Best SVM): 0.6032\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf', 'sigmoid'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Create the SVM model\n",
    "svm_model = SVC(random_state=42)\n",
    "\n",
    "# Perform Grid Search to find the best hyperparameters\n",
    "grid_search_svm = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "grid_search_svm.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(grid_search_svm.best_params_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_svm_model = grid_search_svm.best_estimator_\n",
    "y_pred_test_svm_best = best_svm_model.predict(X_test)\n",
    "test_acc_svm_best = accuracy_score(y_test, y_pred_test_svm_best)\n",
    "print(f\"Test acc (Best SVM): {test_acc_svm_best:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create and train the Decision Tree model\n",
    "dt = DecisionTreeClassifier(max_depth=5)\n",
    "_ = dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc (Decision Tree): 0.5227\n"
     ]
    }
   ],
   "source": [
    "y_pred_train_dt = dt.predict(X_train)\n",
    "train_acc_dt = accuracy_score(y_train, y_pred_train_dt)\n",
    "print(f\"Train acc (Decision Tree): {train_acc_dt:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc (Decision Tree): 0.5149\n"
     ]
    }
   ],
   "source": [
    "y_pred_test_dt = dt.predict(X_test)\n",
    "test_acc_dt = accuracy_score(y_test, y_pred_test_dt)\n",
    "print(f\"Test acc (Decision Tree): {test_acc_dt:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Decision Tree):\n",
      "[[683 192 416]\n",
      " [175 909 644]\n",
      " [236 596 806]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix for Decision Tree\n",
    "confusion_matrix_dt = confusion_matrix(y_test, y_pred_test_dt)\n",
    "print(\"Confusion Matrix (Decision Tree):\")\n",
    "print(confusion_matrix_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning on Decision Tree:\n",
    "\n",
    "max_depth: The maximum depth of the tree. We will test values of 5, 10, and 15 to control the complexity of the model and avoid overfitting.\n",
    "\n",
    "min_samples_split: The minimum number of samples required to split an internal node. We will test values of 2, 5, and 10 to determine the optimal value for splitting nodes.\n",
    "\n",
    "min_samples_leaf: The minimum number of samples required to be at a leaf node. We will test values of 1, 2, and 4 to control overfitting by setting a minimum threshold for samples at leaf nodes.\n",
    "\n",
    "max_features: The number of features to consider when looking for the best split. We will test values of 'sqrt', 'log2', and None (considering all features) to determine the optimal number of features to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "{'max_depth': 5, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Test acc (Best Decision Tree): 0.5151\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Create the Decision Tree model\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Perform Grid Search to find the best hyperparameters\n",
    "grid_search_dt = GridSearchCV(estimator=dt_model, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "grid_search_dt.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(grid_search_dt.best_params_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_dt_model = grid_search_dt.best_estimator_\n",
    "y_pred_test_dt_best = best_dt_model.predict(X_test)\n",
    "test_acc_dt_best = accuracy_score(y_test, y_pred_test_dt_best)\n",
    "print(f\"Test acc (Best Decision Tree): {test_acc_dt_best:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Create and train the AdaBoost model\n",
    "ada = AdaBoostClassifier(n_estimators=50, learning_rate=1.0)\n",
    "_ = ada.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc (AdaBoost): 0.5482\n"
     ]
    }
   ],
   "source": [
    "y_pred_train_ada = ada.predict(X_train)\n",
    "train_acc_ada = accuracy_score(y_train, y_pred_train_ada)\n",
    "print(f\"Train acc (AdaBoost): {train_acc_ada:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc (AdaBoost): 0.5265\n"
     ]
    }
   ],
   "source": [
    "y_pred_test_ada = ada.predict(X_test)\n",
    "test_acc_ada = accuracy_score(y_test, y_pred_test_ada)\n",
    "print(f\"Test acc (AdaBoost): {test_acc_ada:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (AdaBoost):\n",
      "[[ 766  230  295]\n",
      " [ 237 1015  476]\n",
      " [ 306  661  671]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix for AdaBoost\n",
    "confusion_matrix_ada = confusion_matrix(y_test, y_pred_test_ada)\n",
    "print(\"Confusion Matrix (AdaBoost):\")\n",
    "print(confusion_matrix_ada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning on AdaBoost:\n",
    "\n",
    "n_estimators: The number of base estimators. We will test values of 50, 100, and 150 to find the optimal number of estimators.\n",
    "\n",
    "learning_rate: The learning rate shrinks the contribution of each classifier. We will test values of 0.1, 0.5, and 1 to find the optimal learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "{'learning_rate': 0.5, 'n_estimators': 150}\n",
      "Test acc (Best AdaBoost): 0.5585\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.1, 0.5, 1]\n",
    "}\n",
    "\n",
    "# Create the AdaBoost model\n",
    "adaboost_model = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "# Perform Grid Search to find the best hyperparameters\n",
    "grid_search_adaboost = GridSearchCV(estimator=adaboost_model, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "grid_search_adaboost.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(grid_search_adaboost.best_params_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_adaboost_model = grid_search_adaboost.best_estimator_\n",
    "y_pred_test_adaboost_best = best_adaboost_model.predict(X_test)\n",
    "test_acc_adaboost_best = accuracy_score(y_test, y_pred_test_adaboost_best)\n",
    "print(f\"Test acc (Best AdaBoost): {test_acc_adaboost_best:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Create and train the XGBoost model\n",
    "xgboost_model = xgb.XGBClassifier(objective='multi:softmax', num_class=3, max_depth=5, n_estimators=100)\n",
    "_ = xgboost_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc (XGBoost): 0.9586\n"
     ]
    }
   ],
   "source": [
    "y_pred_train_xgb = xgboost_model.predict(X_train)\n",
    "train_acc_xgb = accuracy_score(y_train, y_pred_train_xgb)\n",
    "print(f\"Train acc (XGBoost): {train_acc_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc (XGBoost): 0.5954\n"
     ]
    }
   ],
   "source": [
    "y_pred_test_xgb = xgboost_model.predict(X_test)\n",
    "test_acc_xgb = accuracy_score(y_test, y_pred_test_xgb)\n",
    "print(f\"Test acc (XGBoost): {test_acc_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (XGBoost):\n",
      "[[ 812  216  263]\n",
      " [ 151 1071  506]\n",
      " [ 184  564  890]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix for XGBoost\n",
    "confusion_matrix_xgb = confusion_matrix(y_test, y_pred_test_xgb)\n",
    "print(\"Confusion Matrix (XGBoost):\")\n",
    "print(confusion_matrix_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning on XGBoost:\n",
    "\n",
    "n_estimators: The number of boosting rounds. We will test values of 50, 100, and 150 to find the optimal number of estimators.\n",
    "\n",
    "learning_rate: Step size shrinkage used to prevent overfitting. We will test values of 0.1, 0.5, and 1 to find the optimal learning rate.\n",
    "\n",
    "max_depth: Maximum depth of a tree. We will test values of 3, 5, and 7 to control the complexity of the model and avoid overfitting.\n",
    "\n",
    "subsample: Subsample ratio of the training instances. We will test values of 0.6, 0.8, and 1.0 to determine the optimal subsample ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Test acc (Best XGBoost): 0.6004\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.1, 0.5, 1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Create the XGBoost model\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "\n",
    "# Perform Grid Search to find the best hyperparameters\n",
    "grid_search_xgb = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(grid_search_xgb.best_params_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_xgb_model = grid_search_xgb.best_estimator_\n",
    "y_pred_test_xgb_best = best_xgb_model.predict(X_test)\n",
    "test_acc_xgb_best = accuracy_score(y_test, y_pred_test_xgb_best)\n",
    "print(f\"Test acc (Best XGBoost): {test_acc_xgb_best:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Discussion:\n",
    "\n",
    "Firsly let us explain why we chose accuracy as our principle parameter:<br>Because accuracy is easily interpreted, aligned with its intended purpose, appropriate for balanced classes, and allows for easy model comparison, it was selected as the main parameter to assess the model's effectiveness in predicting gender based on descriptions from Twitter profiles. Stakeholders can easily comprehend accuracy since it gives a clear indication of the proportion of gender labels that were successfully predicted out of all the guesses. Giving each gender category equal weight and accurately defining gender is directly aligned with the purpose of delivering a fair assessment of model performance across all classes. While other metrics like precision, recall, F1-score, or AUC-ROC may be more suited in circumstances with imbalanced datasets or changing misclassification costs, accuracy is sufficient for this task.\n",
    "\n",
    "Random Forest:<br>Prior to hyperparameter adjustment, the Random Forest model demonstrated a modest level of performance, with a test accuracy of 53.53%. Following adjustment, the accuracy increased to 58.32%, showing that the model benefited from having its parameters optimized for more accurate prediction. This improvement shows that gender can be predicted using Random Forest's analysis of profile descriptions.\n",
    "\n",
    "Logistic Regression:<br>Logistic Regression yielded a test accuracy of 59.05%, which was somewhat better than Random Forest. Although hyperparameter tuning did not increase the performance, Logistic Regression's analysis of text data implies that it can accurately determine gender.\n",
    "\n",
    "K-Nearest Neighbors(KNN):<br>KNN showed a high test accuracy of 49.04%, suggesting possible overfitting, but a poor train accuracy of 65.94%. The test accuracy increased to 51.23% after hyperparameter adjustment, although it was still not as high as other models. Based on its performance, KNN might not be the ideal option for this text classification assignment.\n",
    "\n",
    "Support Vector Machine(SVM):<br>Prior to hyperparameter adjustment, SVM demonstrated a reasonable level of performance, with a test accuracy of 57.38%. After Hyperparameter tuning, the accuracy increased to 60.32%, suggesting that SVM is a useful tool for reliably predicting gender in text data analysis.\n",
    "\n",
    "Decision Tree:<br>Decision Tree performed poorly, achieving 51.49% on the test. The test accuracy was not considerably increased by hyperparameter tuning, indicating that Decision Tree may not be the best model for this particular task.\n",
    "\n",
    "AdaBoost:<br>AdaBoost achieved a test accuracy of 52.65% prior to tuning, which was comparable to Decision Tree's performance. Following adjustment, the accuracy increased to 55.85%, suggesting a slight improvement. Based on its performance, AdaBoost might not be the best model for this particular type of task.\n",
    "\n",
    "XGBoost:<br>With a train accuracy of 95.66%, XGBoost showed the highest performance, which may indicate overfitting. After some fine-tuning, the initial model's test accuracy increased to 60.04% from 59.54%. Based on its performance, XGBoost appears to be able to evaluate text input efficiently and determine gender accurately.\n",
    "\n",
    "### Conclusion: \n",
    "\n",
    "With test accuracies of about 60%, the models assessed in this study that showed most potential for determining gender based on descriptions of Twitter profiles were Logistic Regression, Support Vector Machine (SVM), and XGBoost. These models showed a better degree of effectiveness in accurately predicting gender based on text data analysis. The selection of accuracy as the performance parameter was suitable for this task since it offered a transparent indicator of the model's effectiveness in a multi-class classification context. This study has other possible use cases and outcomes beyond its ability to guess the gender of Twitter account holders based on their profile descriptions. For example:\n",
    "\n",
    "Social Media Analysis: Social media analytics firms can use this technique to learn more about the demographics of Twitter users. Use of this data for marketing and advertising campaigns may be beneficial.\n",
    "\n",
    "Targeted Marketing: Based on the gender distribution of their target audience on Twitter, businesses can use this model to more successfully target their marketing initiatives.\n",
    "\n",
    "Brand Perception Analysis: Businesses can learn more about how different genders view their brand by examining the gender distribution of followers and how they interact with the social media accounts of brands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By\n",
    "#### Sunil Yanagandula<br>Kumara Swamy Padigeri"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
